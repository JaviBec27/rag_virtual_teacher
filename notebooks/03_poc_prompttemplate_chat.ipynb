{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40be1a9-fe0a-4b86-a893-440cb824ee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key cargada correctamente.\n",
      "Cargando la base de datos vectorial desde 'faiss_index_doc1'...\n",
      "\n",
      "¡El Dr. Deo está listo para responder preguntas!\n",
      "Escribe 'salir' para terminar la conversación.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# Nuevas importaciones para la sintaxis moderna (LCEL)\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# --- Carga de la API key (esto está perfecto) ---\n",
    "try:\n",
    "    with open(\"../gem_apikey.txt\") as f:\n",
    "        api_key = f.read().strip()\n",
    "    print(\"API key cargada correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontró el archivo '../gem_apikey.txt'.\")\n",
    "    api_key = None\n",
    "# -----------------------------------------\n",
    "\n",
    "\n",
    "def iniciar_chat():\n",
    "    \"\"\"\n",
    "    Carga la base de datos vectorial y permite chatear con la personalidad del Dr. Deo\n",
    "    utilizando la sintaxis moderna de LangChain (LCEL).\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        print(\"Proceso detenido. No se pudo cargar la API key.\")\n",
    "        return\n",
    "\n",
    "    # --- CONFIGURACIÓN ---\n",
    "    CARPETA_GUARDADO = \"faiss_index_doc1\"\n",
    "\n",
    "    # 1. Carga de la base de datos vectorial\n",
    "    print(f\"Cargando la base de datos vectorial desde '{CARPETA_GUARDADO}'...\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\", google_api_key=api_key\n",
    "    )\n",
    "    vectorstore = FAISS.load_local(\n",
    "        CARPETA_GUARDADO, embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    # Se define el retriever una sola vez\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # 2. Configuración del LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\", temperature=0.3, google_api_key=api_key\n",
    "    )\n",
    "\n",
    "    # 3.1 Prompt con personalidad y contexto\n",
    "    system_tamplate = \"\"\"Eres un asistente virtual que se comporta como un profesor de {knowledgeDomain} experto.\n",
    "    Tu nombre es \"IAsistente de {Teacher}\". Eres amable, didáctico y te encanta {knowledgeDomain}. \n",
    "    REGLA ESTRICTA: Solo puedes responder preguntas relacionadas con {knowledgeDomain} basándote en el contexto proporcionado. Si la pregunta no está relacionada con estos temas, \n",
    "    responde: \"Lo siento, mi especialidad es {knowledgeDomain}. No puedo responder preguntas sobre otros temas.\"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_tamplate)\n",
    "    system_message_prompt.input_variables\n",
    "\n",
    "    # 3.2 Prompt para la pregunta del usuario\n",
    "    human_template = \"{pregunta_usuario}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    human_message_prompt.input_variables\n",
    "\n",
    "    # 3.3 Combinación de los prompts en un ChatPromptTemplate\n",
    "\n",
    "    plantilla_chat = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt, human_message_prompt]\n",
    "    )\n",
    "\n",
    "    # 3.4 Definición de las variables del sistema\n",
    "    plantilla_chat = plantilla_chat.partial(Teacher=\"Dr. Deo\", knowledgeDomain=\"Medicina\")\n",
    "\n",
    "    # 4. Creación de la cadena de RAG completa con LCEL\n",
    "    #    Esta es la parte principal de la corrección.\n",
    "    cadena_rag = (\n",
    "        {\"contexto\": retriever, \"pregunta_usuario\": RunnablePassthrough()}\n",
    "        | plantilla_chat\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"\\n¡El Asistenteestá listo para responder preguntas!\")\n",
    "    print(\"Escribe 'salir' para terminar la conversación.\")\n",
    "\n",
    "\n",
    "    # 5. Bucle de chat\n",
    "    while True:\n",
    "        query = input(\"\\nPregunta del estudiante: \")\n",
    "        if query.lower() == \"salir\":\n",
    "            break\n",
    "\n",
    "        # Invocamos la cadena completa con la pregunta del usuario\n",
    "        respuesta = cadena_rag.invoke(query)\n",
    "\n",
    "        print(\"\\nRespuesta del Asistente:\")\n",
    "        # La respuesta ya es un texto limpio gracias a StrOutputParser\n",
    "        print(respuesta)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iniciar_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
